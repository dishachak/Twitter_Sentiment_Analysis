# -*- coding: utf-8 -*-
"""30-Apr-2022 Tweet Sentiment Analysis Ver 1.0

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iDM_8UErxt-etS8Cus2wsyZOJSOHVUgr
"""

#Author- Disha Chakraborty
#30-Apr-2022 Tweet Sentiment Analysis Ver 1.0 last updated: 30-April-2022

!pip install -q snscrape
!pip install nltk
!pip install spacy
!python -m spacy download en_core_web_sm



# Import the libraries required in the program
import os
import pandas as pd
from datetime import date
import nltk
nltk.download('vader_lexicon')
import re
import spacy
spacy.load('en_core_web_sm')

today= date.today()
end_date=today
print(end_date)

search_term= 'Russia-Ukraine War'
from_date='2022-02-23'

#Total number of tweets containing the search term
os.system(f"snscrape --since {from_date} twitter-search '{search_term} until:{end_date}' > result-tweets.txt")
if os.stat("result-tweets.txt").st_size==0:
    counter=0
else:
    df1=pd.read_csv('result-tweets.txt',names=['link'])
    counter=df1.size


print('Number Of Tweets : '+str(counter))

#First define the maximum number of tweets that we require
max_results=6000

#Extracting the exact tweets
extracted_tweets= "snscrape --format '{content!r}'" + f" --max-results {max_results} --since {from_date} twitter-search '{search_term} until:{end_date}' > extracted-tweets.txt"
os.system(extracted_tweets)
if os.stat("extracted-tweets.txt").st_size==0:
  print('No Tweets found')
else:
  df2= pd.read_csv('extracted-tweets.txt', names=['content'])
  read_file= pd.read_csv('extracted-tweets.txt', names=['content']) # Reading the txt file as a csv file
  read_file.to_csv('extract.csv',index=None)  # Writing the csv file to extract.csv
  df=pd.read_csv('extract.csv')  # Reading the csv file and stroing in the dataframe df to process the tweets further
  print('Number Of Tweets Selected finally : ' +str(len(df)))

# Function for removing user name and URL from text  using regex

def remove_pattern(input_txt):
    input_txt = re.sub(r"\S*https?:\S*",'',input_txt)
    input_txt = re.sub('@[^\s]+','',input_txt)
    x = ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",input_txt).split())

    return x

# Function to select relevant tweets that concern Russia Ukraine war
war_list = [ 'war','weapons','attack']

def check_war(input_txt):
    flag = 'N'
    x = input_txt.lower()

    for war in war_list:
        if war in x:
             flag = 'Y'

    return flag

# Performing sentiment analysis on each of the tweets using Vader from NLTK
from nltk.sentiment.vader import SentimentIntensityAnalyzer

def sentiment_scores(sentence):

    # Create a SentimentIntensityAnalyzer object.
    sid_obj = SentimentIntensityAnalyzer()
    sentiment_dict = sid_obj.polarity_scores(sentence)

   # decide sentiment as positive, negative and neutral
    if sentiment_dict['compound'] >=0.05:
      x="Positive"

    elif sentiment_dict['compound'] <=-0.05:
      x="Negative"

    else :
      x="Neutral"

    return x

#Processing the text of the tweets
#Read all the tweets and for each of them, remove tweet handle, URL, hashtags

clean_txt= []

for row in df['content'].iteritems():
    clean_txt.append(remove_pattern(row[1]))

df['Tweet']= clean_txt
df.drop(['content'],axis=1,inplace=True)

#Removing rows with blank values
df.dropna(subset=['Tweet'])

#Computing the length of each of the processed tweets
#selecting tweets with 100 or more characters for further processing of meaningful tweets
df['length_txt'] = df['Tweet'].apply(lambda x: len(x))
df = df[df['length_txt'] >= 100]

#For each tweet, checking if the war-related words are present or not
at_war = []
for index, row in df.iterrows():
    at_war.append(check_war(row['Tweet']))

#select those tweets that have war-related contents
df['Tweet of War?'] = at_war
df = df[df['Tweet of War?'] == 'Y']

#Calculation of sentiment score for each tweet using vader
#Possible values- positive/negative/neutral
df['Sentiment'] = df['Tweet'].apply(lambda x: sentiment_scores(x))

#Select tweet and sentiment columns only from the Dataframe
df = df[['Tweet','Sentiment']]

#writing the dataframe into a csv file
df.to_csv('2.0_tweet_sentiment.csv', encoding='utf-8', index=False)

!pip install -U textblob
!python -m textblob.download_corpora

import pandas as pd
import numpy  as np
import nltk
nltk.download('stopwords')
import re
import string
from nltk.corpus import wordnet
from nltk.corpus import stopwords
stopWords = set(stopwords.words('english'))

from nltk.corpus import wordnet_ic
from textblob import TextBlob
from nltk.stem import PorterStemmer

import warnings
warnings.filterwarnings('ignore')

digits_list = ['1','2','3','4','5','6','7','8','9','0']
my_stop_words = []

# Read the file, exclude all 'Neutral' and encode Positive=1 and Negative=0
df=pd.read_csv('2.0_tweet_sentiment.csv')
df = df[df['Sentiment'] != 'Neutral']
df['label'] = df['Sentiment'].apply(lambda x: 1 if x=='Positive' else 0)

df.head()

exclude = set(string.punctuation).union(set(digits_list))
stop = set(stopwords.words('english')).union(set(my_stop_words))

from nltk.stem import PorterStemmer
ps = PorterStemmer()

def clean(comment,flag):
    try:
        punc_free = ''.join(ch for ch in comment if ch not in exclude)
        stop_free = ' '.join([i for i in punc_free.lower().split() if i not in stop])

        if flag =='Stem':
            stem_free = " ".join([ps.stem(i) for i in stop_free.split()])
            return stem_free
        else:
            return stop_free
    except:
        punc_free, stop_free, stem_free = '','', ''

        if flag =='Stem':
            return stem_free
        else:
            return stop_free

df['Tweet_stem'] = df['Tweet'].apply(lambda x: clean(x,'Stem'))
df.head()

# https://vitalflux.com/text-classification-bag-of-words-model-python-sklearn/
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(max_features=2000)

docs = df['Tweet_stem'].to_numpy()

# Fit the bag-of-words model
bag = vectorizer.fit_transform(docs)

# Get unique words / tokens found in all the documents. The unique words / tokens represents
# the features
#
#print(vectorizer.get_feature_names())
#
# Associate the indices with each unique word
#
#print(vectorizer.vocabulary_)
#
# Print the numerical feature vector
#
#print(bag.toarray())

# print(set(vectorizer.get_feature_names()))
len(set(vectorizer.get_feature_names()))

#Creating training data set from bag-of-words  and dummy label
X = bag.toarray()
y = df['label'].to_numpy()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics

X_train, X_test, y_train, y_test = train_test_split(X, y)
lr = LogisticRegression(C=1.0, random_state=1, solver='lbfgs', multi_class='ovr')
lr.fit(X_train, y_train)
y_predict = lr.predict(X_test)

# Use metrics.accuracy_score to measure the score
print("LogisticRegression Accuracy %.3f" %metrics.accuracy_score(y_test, y_predict))
print(metrics.confusion_matrix(y_test,y_predict))
print(classification_report(y_test,y_predict))

def get_shingles(my_words):
    two_words = []

    ngram_object = TextBlob(my_words)
    bigram = ngram_object.ngrams(n=2)

    i=0
    while i < len(bigram):
        z = bigram[i]
        y = list(set([' '.join([z[0],z[1]])]))
        i=i+1
        two_words.extend(y)
    return two_words

df['Tweet_bigram'] = df['Tweet'].apply(lambda x: get_shingles(clean(x,'Stop')))

#Find out who all are in support of Russia and who all support Ukraine using bigram
support_ukraine = ['help ukraine','zelenskiy vows', 'stop russia', 'stop putin','monster putin','putin attacks','collateral damage','ukraine negotiations','shuts crypto','russian attack','suffers setbacks','join nato']
support_russia  = ['support russia','embattling russia','continued war']

def check_support(bigram_list):
    flag = 'None'

    for item in support_ukraine:
        if item in bigram_list:
             flag = 'Ukraine'
        else:
            for item in support_russia:
                if item in bigram_list:
                    flag = 'Russia'

    return flag

df['Support'] = df['Tweet_bigram'].apply(lambda x: check_support(x))
df.head(10)

"""Classification using Decision Tree Classifier"""

from sklearn.model_selection import train_test_split

X = bag.toarray()
y = df['label'].to_numpy()

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)

from sklearn.tree import DecisionTreeClassifier
dtree= DecisionTreeClassifier()
dtree.fit(X_train,y_train)

predictions= dtree.predict(X_test)

# Use metrics.accuracy_score to measure the score
print("DecisionTreeClassifier Accuracy %.3f" %metrics.accuracy_score(y_test, predictions))
print(metrics.confusion_matrix(y_test,predictions))
from sklearn.metrics import classification_report
print(classification_report(y_test,predictions))



"""Classification using RandomForestClassifier"""

#RandomForest is a bunch of decision trees and is an ensemble of multiple decision trees
from sklearn.ensemble import RandomForestClassifier
rfc= RandomForestClassifier(n_estimators=200)
rfc.fit(X_train,y_train)

rfc_pred= rfc.predict(X_test)

# Use metrics.accuracy_score to measure the score
print("RandomForestClassifier Accuracy %.3f" %metrics.accuracy_score(y_test, rfc_pred))
print(metrics.confusion_matrix(y_test,rfc_pred))
print(classification_report(y_test,rfc_pred))

print(classification_report(y_test,rfc_pred))

"""Classification using Support Vector Machine"""

from sklearn.svm import SVC

model=SVC()

model.fit(X_train,y_train)

svc_pred= model.predict(X_test)

# Use metrics.accuracy_score to measure the score
print("SupportVectorClassifier Accuracy %.3f" %metrics.accuracy_score(y_test, svc_pred))
print(metrics.confusion_matrix(y_test,svc_pred))
print(classification_report(y_test,svc_pred))

print(classification_report(y_test,svc_pred))

"""Classification using KNN"""

from sklearn.neighbors import KNeighborsClassifier
knn= KNeighborsClassifier(n_neighbors=1)

knn.fit(X_train,y_train)

knn_pred=knn.predict(X_test)

# Use metrics.accuracy_score to measure the score
print("KNNClassifier Accuracy %.3f" %metrics.accuracy_score(y_test, knn_pred))
print(metrics.confusion_matrix(y_test,knn_pred))
print(classification_report(y_test,knn_pred))

print(classification_report(y_test,knn_pred))

